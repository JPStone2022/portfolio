{# demos/templates/demos/data_security_demo.html #}
{% extends 'portfolio/base.html' %} {# Assumes base is in portfolio app #}
{% load static %}
{% load humanize %}

{% block title %}{{ page_title|default:"Security in ML, AI & Data Science" }} - Portfolio{% endblock %}
{% block meta_description %}{{ meta_description|default:"Understanding key security risks and considerations in machine learning, AI, and data science, including data privacy, model security, and infrastructure protection." }}{% endblock %}
{% block meta_keywords %}{{ meta_keywords|default:"security, machine learning, data science, AI, data privacy, model security, adversarial attacks, infrastructure security, GDPR, CCPA, OWASP" }}{% endblock %}


{% block content %}
<div class="container mx-auto px-6 py-12">
    {# Apply gradient text to heading #}
    <h1 class="text-4xl md:text-5xl font-bold text-center mb-6 bg-gradient-to-r from-red-500 via-rose-500 to-pink-600 dark:from-red-400 dark:via-rose-400 dark:to-pink-500 bg-clip-text text-transparent">
        {{ page_title|default:"Security in ML, AI & Data Science" }}
    </h1>
    <p class="text-center text-gray-600 dark:text-gray-400 max-w-3xl mx-auto mb-10">
        As Machine Learning, AI, and Data Science systems become more powerful and handle sensitive information, <strong>security</strong> becomes paramount. It's not just about protecting the code, but also the data, the models themselves, and the infrastructure they run on. Ignoring security can lead to data breaches, biased or manipulated outcomes, and loss of trust.
    </p>

    {# Main content area #}
    <div class="max-w-4xl mx-auto bg-white dark:bg-gray-800 p-8 rounded-lg shadow-lg dark:shadow-rose-900/20 transition-colors duration-300 ease-in-out">

        {# --- Section 1: Data Privacy & Security --- #}
        <section class="mb-8 pb-6 border-b border-gray-200 dark:border-gray-700">
            <h2 class="text-2xl font-semibold text-gray-800 dark:text-gray-100 mb-3">1. Data Privacy & Security</h2>
            <div class="prose prose-sm prose-indigo dark:prose-invert max-w-none text-gray-700 dark:text-gray-300 leading-relaxed mb-4 space-y-3"> {# Added space-y-3 #}
                <p>ML models are trained on data, which often contains sensitive or personal information (PII). Protecting this data is critical.</p>
                <p><strong>Key Risks:</strong></p>
                <ul class="!mt-0"> {# Adjust margin if needed #}
                    <li>Data breaches during storage or transit.</li>
                    <li>Unauthorized access by internal or external actors.</li>
                    <li>Models inadvertently memorizing and potentially leaking sensitive training data points through their outputs (inference attacks).</li>
                    <li>Non-compliance with data protection regulations.</li>
                </ul>
                <p><strong>Mitigation Techniques:</strong></p>
                <ul class="!mt-0">
                    <li><strong>Anonymization/Pseudonymization:</strong> Removing or replacing direct identifiers (e.g., replacing names with IDs). However, re-identification can still be possible through linkage.</li>
                    <li><strong>Access Control:</strong> Implementing strict Role-Based Access Control (RBAC) and Principle of Least Privilege for data access.</li>
                    <li><strong>Encryption:</strong> Using strong encryption for data at rest (in databases, file storage) and in transit (using TLS/SSL).</li>
                    <li><strong>Differential Privacy:</strong> Adding mathematically calibrated noise during training or analysis to provide formal privacy guarantees, making it difficult to infer information about specific individuals. (<a href="https://desfontain.es/privacy/differential-privacy-awesomely-explained.html" target="_blank" rel="noopener noreferrer" class="text-blue-600 dark:text-blue-400 hover:underline">Learn More - External Link</a>)</li>
                    <li><strong>Compliance:</strong> Designing systems and processes to adhere to regulations like GDPR, CCPA, HIPAA. (<a href="https://gdpr-info.eu/" target="_blank" rel="noopener noreferrer" class="text-blue-600 dark:text-blue-400 hover:underline">GDPR Info</a>)</li>
                    <li><strong>Data Minimization:</strong> Collecting and retaining only the data that is strictly necessary for the intended purpose.</li>
                </ul>
                <p><strong>Importance:</strong> Protecting user privacy is often a legal and ethical requirement, crucial for maintaining user trust and avoiding significant penalties.</p>
            </div>
        </section>

        {# --- Section 2: Model Security --- #}
        <section class="mb-8 pb-6 border-b border-gray-200 dark:border-gray-700">
            <h2 class="text-2xl font-semibold text-gray-800 dark:text-gray-100 mb-3">2. Model Security & Robustness</h2>
            <div class="prose prose-sm prose-indigo dark:prose-invert max-w-none text-gray-700 dark:text-gray-300 leading-relaxed mb-4 space-y-3">
                <p>Machine learning models themselves can be vulnerable to specific types of attacks designed to manipulate their behavior or extract information.</p>
                <p><strong>Key Risks:</strong></p>
                <ul class="!mt-0">
                    <li><strong>Adversarial Attacks (Evasion):</strong> Crafting specific, often subtle, inputs designed to fool a model into making incorrect predictions. <em>(e.g., adding tiny, almost invisible noise to an image of a 'stop' sign might cause a self-driving car's model to classify it as a 'speed limit' sign).</em></li>
                    <li><strong>Data Poisoning (Causative Attack):</strong> Injecting malicious data into the training set to compromise the resulting model's performance, introduce biases, or create backdoors triggered by specific inputs.</li>
                    <li><strong>Model Inversion/Extraction:</strong> Trying to reconstruct sensitive information about the training data (membership inference) or steal the model's architecture and parameters (model stealing) by repeatedly querying its API.</li>
                </ul>
                <p><strong>Mitigation Techniques:</strong></p>
                <ul class="!mt-0">
                    <li><strong>Adversarial Training:</strong> Including carefully crafted adversarial examples during the training process to improve model robustness against evasion attacks.</li>
                    <li><strong>Input Validation/Sanitization:</strong> Rigorously checking, cleaning, and normalizing inputs before feeding them to the model to detect or remove potential adversarial perturbations.</li>
                    <li><strong>Data Provenance & Filtering:</strong> Carefully vetting training data sources and implementing filters to detect potentially poisoned samples.</li>
                    <li><strong>Defensive Distillation:</strong> Training a model on the softened probability outputs of another model trained on the same task, which can sometimes increase robustness.</li>
                    <li><strong>Output Perturbation/Randomization:</strong> Adding noise to model outputs or using ensemble methods can make model extraction harder.</li>
                    <li><strong>Monitoring & Anomaly Detection:</strong> Implementing systems to detect unusual input patterns, query frequencies, or prediction distributions that might indicate an attack.</li>
                    <li><strong>Rate Limiting & Access Control for Model API:</strong> Limiting query rates and restricting access to the model's prediction endpoint.</li>
                </ul>
                 <p><strong>Importance:</strong> Ensures the reliability, integrity, and trustworthiness of the AI system's predictions and decisions, preventing manipulation and protecting intellectual property.</p>
            </div>
        </section>

        {# --- Section 3: Infrastructure & Deployment Security --- #}
        <section class="mb-8 pb-6 border-b border-gray-200 dark:border-gray-700">
            <h2 class="text-2xl font-semibold text-gray-800 dark:text-gray-100 mb-3">3. Infrastructure & Deployment Security</h2>
            <div class="prose prose-sm prose-indigo dark:prose-invert max-w-none text-gray-700 dark:text-gray-300 leading-relaxed mb-4 space-y-3">
                <p>The systems supporting ML models (servers, APIs, databases, cloud environments) require standard, robust security practices.</p>
                 <p><strong>Key Risks:</strong></p>
                 <ul class="!mt-0">
                    <li>Unauthorized access to servers, cloud accounts, or container orchestration systems.</li>
                    <li>Insecure API endpoints lacking proper authentication, authorization, or input validation.</li>
                    <li>Vulnerabilities in third-party dependencies (Python packages, OS libraries).</li>
                    <li>Denial-of-service (DoS/DDoS) attacks against model APIs or supporting infrastructure.</li>
                    <li>Insecure storage or transmission of model artifacts or credentials.</li>
                 </ul>
                 <p><strong>Mitigation Techniques:</strong></p>
                 <ul class="!mt-0">
                    <li><strong>Secure Coding Practices:</strong> Applying general web security principles (like OWASP Top 10) to any API or web interface serving the model. (<a href="https://owasp.org/www-project-top-ten/" target="_blank" rel="noopener noreferrer" class="text-blue-600 dark:text-blue-400 hover:underline">OWASP Top 10</a>)</li>
                    <li><strong>Infrastructure Security:</strong> Using firewalls, Virtual Private Clouds (VPCs), secure network configurations, regular OS/platform patching, and infrastructure vulnerability scanning.</li>
                    <li><strong>API Security Best Practices:</strong> Implementing strong authentication (e.g., API keys, OAuth), fine-grained authorization, robust input validation, rate limiting, and logging for model APIs.</li>
                    <li><strong>Dependency Management & Scanning:</strong> Regularly updating libraries and using tools (like `pip-audit`, Snyk, Dependabot) to scan for known vulnerabilities in dependencies.</li>
                    <li><strong>Secrets Management:</strong> Securely storing API keys, database passwords, and other credentials using environment variables injected at runtime or dedicated secrets management services (e.g., HashiCorp Vault, AWS Secrets Manager, Google Secret Manager). <em>(e.g., storing database passwords directly in code or version control is highly insecure).</em></li>
                    <li><strong>Secure Containerization Practices:</strong> Using minimal base images, scanning container images for vulnerabilities, and running containers with least privilege.</li>
                 </ul>
                 <p><strong>Importance:</strong> Protects the entire system surrounding the model, ensuring its availability, preventing unauthorized access or disruption, and safeguarding underlying data and infrastructure.</p>
        </section>

         {# --- Conclusion --- #}
         <section>
            <h2 class="text-2xl font-semibold text-gray-800 dark:text-gray-100 mb-3">Conclusion: A Holistic Approach</h2>
            <div class="prose prose-sm prose-indigo dark:prose-invert max-w-none text-gray-700 dark:text-gray-300 leading-relaxed">
                <p>Security in ML/AI/DS is not an afterthought but a critical component throughout the entire lifecycle. It requires a holistic approach encompassing:</p>
                <ul class="!mt-0">
                    <li>Robust **data governance and privacy** measures from collection to disposal.</li>
                    <li>Techniques to enhance **model robustness** against adversarial manipulation and information leakage.</li>
                    <li>Standard, rigorous **infrastructure and application security** practices for deployment and operation.</li>
                </ul>
                <p>As these systems become more integrated into critical applications, adopting a security-conscious mindset and continuously evaluating potential threats is essential for building trustworthy, reliable, and ethical AI.</p>
            </div>
        </section>

    </div>

</div>

 <div class="text-center mt-12">
    <a href="{% url 'portfolio:index' %}" class="text-blue-600 dark:text-blue-400 hover:underline focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-blue-500 dark:focus:ring-offset-gray-900 rounded">&larr; Back to Home</a>
</div>
</div>
{% endblock %}
